{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "f82ab70f6e4e0673e87f3f6960512c9807b8b91e8473aea7edbc34f463490a2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.任务描述\n",
    "在医学搜索中，对搜索问题的意图分类可以极大提升搜索结果的相关性，特别是医学知识具备极强的专业性，对问题意图进行分类也有助于融入医学知识来做增强搜索结果的性能。本任务数据集就是在这样的背景下产生的。\n",
    "\n",
    "## 2.任务说明\n",
    "在本次评测中，医学问题分为 病情诊断(diagnosis）、病因分析(cause)、治疗方案(method)、就医建议(advice)、指标解读(metric_explain)、疾病描述(disease_express)、后果表述(result)、注意事项(attention)、功效作用(effect)、医疗费用(price)、其他(other) 共11种类型，类型说明和示例如下：\n",
    "\n",
    "- 病情诊断：已知症状，判断可能的原因， 如：\n",
    "\n",
    "    - 最近早上起来浑身无力是怎么回事？\n",
    "    - 我家宝宝快五个月了，为什么偶尔会吐清水带？\n",
    "- 病因分析：已知疾病，解释疾病发生的原因。如：\n",
    "\n",
    "    - 阴道松弛的原因是什么？\n",
    "    - 鼻咽癌是如何发生的？\n",
    "- 治疗方案：已知疾病/症状，给出治疗或缓解的方案（检查/手术/药物/行为）。如：\n",
    "\n",
    "    - 腰椎间盘突出可以烤电吗\n",
    "    - 感冒头疼吃什么药好\n",
    "    - 宝宝感冒眼屎多又黄怎么办\n",
    "    - 烫伤的疤痕要怎么去除？\n",
    "- 就医建议：已知症状/疾病，给出就医建议（科室/检查）。如：\n",
    "\n",
    "    - 糖尿病该做什么检查？\n",
    "    - 肚子疼去什么科室？\n",
    "- 指标解读：身高/体重/血压等检查结果的数值范围解读。如：\n",
    "\n",
    "    - 血常规超敏C反应蛋白偏高说明什么\n",
    "    - b超检查报告写的检测到盆腔积液是11mm，严重么？\n",
    "- 疾病描述：疾病属性（eg：能不能治、能不能治好）、症状、表现、图片等相关表述。如：\n",
    "\n",
    "    - 外痔疮早期症状有哪些呢？\n",
    "    - 白癜风能不能治愈\n",
    "- 后果表述：疾病/症状/药品/检查项/食物的危害，疾病恶化不治疗会产生的不良影响或治疗后会产生好的结果。如：\n",
    "\n",
    "    - 缺乏钾元素会怎么样\n",
    "    - 乙肝不治疗会怎么样\n",
    "- 注意事项：病人要注意的事情，以及分析食物的好坏，食物对病人的影响。如：\n",
    "\n",
    "    - 哮喘应该注意些什么\n",
    "    - 孕妇能不能吃榴莲\n",
    "    - 柿子不能和什么一起吃\n",
    "    - 糖尿病人饮食注意什么啊？\n",
    "- 功效作用：食品/药物的好处，功效/作用/副作用。如：\n",
    "\n",
    "    - 乌鸡白凤丸的功效和作用\n",
    "    - 玫瑰，柠檬，菊花可以一起泡吗？有什么功效\n",
    "- 医疗费用：疾病/手术/药品/检查/的费用。如：\n",
    "\n",
    "    - 二甲双瓜要多少钱？\n",
    "- 其他：无法涵盖在前面分类里的以及低价值/无意义/非医疗、需求不明没讲明白的。如：\n",
    "\n",
    "    - 玻尿酸丰唇能保持多久\n",
    "    - 血常规五分类是查什么\n",
    "## 3.评测指标\n",
    "本任务的评价指标使用准确率Accuracy来评估，即：\n",
    "\n",
    "准确率(Accuracy) = #预测正确的条目数 / #预测总条目数\n",
    "\n",
    "## 4.评测数据\n",
    "本评测开放训练集数据6931条，验证集数据1955条，测试集数据1994条。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cut\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "# stop_words=list()\n",
    "#  stop_words = list(pd.read_csv(r'stopwords\\total_stopwords.txt',header=None,sep='\\\\n',encoding='utf-8',engine='python')[0])\n",
    "def tokenize(text):\n",
    "#     remove_chars = '[。·’!\"\\#$%&\\'()＃！（）*+,-./:;<=>?\\@，：?￥★、…．＞【】［］《》？“”‘’\\[\\\\]^_`{|}~]+'\n",
    "#     text = re.sub(remove_chars, \"\", text)\n",
    "#     cut = jieba.lcut(text.replace(\" \",\"\"))    \n",
    "#     result = list()\n",
    "#     for word in cut:   \n",
    "#         if word not in stop_words:   \n",
    "#             result.append(word)\n",
    "    result = list(text)\n",
    "    return result"
   ]
  },
  {
   "source": [
    "## 说明\n",
    "### 分词\n",
    "- 这里的注释是最早对文本进行分词处理的方式：\n",
    "    - 使用jieba分词并去除一定的停用词\n",
    "- 但最后在所有任务中都采用了逐字分词的方案\n",
    "    - 这是处于想要屏蔽分词效果的影响\n",
    "    - 同时考虑dnn对于逐字处理可能具有一定优势\n",
    "    - 再是当前这些任务的特殊背景（医疗文本）可能进行逐字处理有优势 \n",
    "        - 一方面在专业术语和日常用语混杂的情况，错误的分词很可能出现，而且一旦出现就可能导致严重的误判\n",
    "        - 另一方面，专业术语中有相当数量的生僻字，而这些生僻字往往可以是很重要的特征\n",
    "            - 其次，这些生僻字数量又是相对有限的\n",
    "            - 并且这些生僻字的顺序调换可能就会指代比较相似但不同的实体\n",
    "    - 但还需要强调的是，以上内容是个人的主观感受，并没有特别严谨的实验验证\n",
    "        - 指一次改了很多变量导致性能提升而没法确定到底哪个是好处哪个是坏处\n",
    "    - 并且其实并不是每个任务以上思路都是适用的\n",
    "        - 一个特别典型的例子就是CHIP-CDN\n",
    "        - 该任务为术语标准化，其中第一个步骤为召回\n",
    "            - 召回采用的是tdidf的特征矩阵加上Cluster pruning\n",
    "            - 在召回步骤中，采用逐字分词完全可能导致非常严重的后果\n",
    "                - 因为tdidf算法是只考虑词频而不考虑词序的，他是一个词袋模型\n",
    "                - 而术语召回的过程中有大量词频十分类似，需要具体词语来判断的词语\n",
    "                - 这就会导致在召回的过程就命中率不高\n",
    "                - 而pipline的方案在非dnn的步骤就可能成为效果的瓶颈\n",
    "        - 上述说明其实对于dnn可能也是适用的，由于dnn的特性而没法下断言\n",
    "        - 并且虽然说了这么多其实也没有特别严谨的实验验证\n",
    "### 其余\n",
    "其余数据处理依旧是和别的任务一般大差不差"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word to sequence\n",
    "UNK_TAG = \"UNK\"\n",
    "PAD_TAG = \"PAD\"\n",
    "class Word2Sequence():\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2index_dict = {\n",
    "            UNK_TAG : self.UNK,\n",
    "            PAD_TAG : self.PAD,\n",
    "        }\n",
    "        self.count = {}\n",
    "\n",
    "\n",
    "    def fit(self, sentence):\n",
    "        # 保存句子到dict\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "    def build_vocab(self,min=0,max=None,max_features=None):\n",
    "        self.count = {word:value for word,value in self.count.items() if value > min}\n",
    "        if(max is not None):\n",
    "            self.count = {word:value for word,value in self.count.items() if value < max}\n",
    "        if max_features is not None:\n",
    "            self.count = dict(sorted(self.count.items(), key = lambda x:x[-1], reverse=True)[:max_features])\n",
    "\n",
    "        for word in self.count:\n",
    "            self.word2index_dict[word] = len(self.word2index_dict)\n",
    "        self.index2word_dict = dict(zip(self.word2index_dict.values(), self.word2index_dict.keys()))\n",
    "\n",
    "\n",
    "    def words2index_transform(self, sentence, max_len=None):\n",
    "        if max_len is not None:\n",
    "            if max_len > len(sentence):\n",
    "                sentence = sentence + [PAD_TAG] * (max_len - len(sentence))\n",
    "            else:\n",
    "                sentence = sentence[:max_len]\n",
    "        return [self.word2index_dict.get(word, self.UNK) for word in sentence]\n",
    "\n",
    "\n",
    "    def index2words_transform(self, sentence):\n",
    "        return [self.index2word_dict.get(index) for index in sentence]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n2405\n"
     ]
    }
   ],
   "source": [
    "# dictionary build\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_data_path = r\"data\\KUAKE-QIC\\KUAKE-QIC_train.json\"\n",
    "test_data_path = r\"data\\KUAKE-QIC\\KUAKE-QIC_test.json\"\n",
    "dev_data_path = r\"data\\KUAKE-QIC\\KUAKE-QIC_dev.json\"\n",
    "if(not os.path.exists(\"models/KUAKE-QIC_Word2Sequence.pkl\")):\n",
    "    word_index_tranformer = Word2Sequence()\n",
    "    with open(train_data_path, encoding=\"utf-8\") as f:\n",
    "        for data in tqdm(json.load(f)):\n",
    "            word_index_tranformer.fit(tokenize(data['query']))\n",
    "    word_index_tranformer.build_vocab()\n",
    "    pickle.dump(word_index_tranformer, open(r\"models/KUAKE-QIC_Word2Sequence.pkl\", 'wb'))\n",
    "else:\n",
    "    word_index_tranformer = pickle.load(open(r\"models/KUAKE-QIC_Word2Sequence.pkl\", 'rb'))\n",
    "print('\\n' + str(len(word_index_tranformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'病情诊断': 0, '病因分析': 1, '治疗方案': 2, '就医建议': 3, '指标解读': 4, '疾病表述': 5, '后果表述': 6, '注意事项': 7, '功效作用': 8, '医疗费用': 9, '其他': 10}\n{0: '病情诊断', 1: '病因分析', 2: '治疗方案', 3: '就医建议', 4: '指标解读', 5: '疾病表述', 6: '后果表述', 7: '注意事项', 8: '功效作用', 9: '医疗费用', 10: '其他'}\n"
     ]
    }
   ],
   "source": [
    "# label encode\n",
    "import numpy as np\n",
    "import torch\n",
    "labels_name = \"\"\"病情诊断\n",
    "病因分析\n",
    "治疗方案\n",
    "就医建议\n",
    "指标解读\n",
    "疾病表述\n",
    "后果表述\n",
    "注意事项\n",
    "功效作用\n",
    "医疗费用\n",
    "其他\"\"\"\n",
    "label2num_dict = {line:index for index, line in enumerate(labels_name.split('\\n'))} \n",
    "num2label_dict = {index:line for index, line in enumerate(labels_name.split('\\n'))} \n",
    "print(label2num_dict)\n",
    "print(num2label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, tensor([2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))\n6931\ntensor([ 599, 1345,    0, 1208,    4,  344,   99,  617,   60,   95, 1230,  283,\n          95,    5,    6,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "\n",
    "max_sentece_length = 20\n",
    "class RosDataset(Dataset):\n",
    "    def __init__(self, data_path, train=True):\n",
    "        self.train = train\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            self.data_list = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        cuted_text = tokenize(self.data_list[index][\"query\"])\n",
    "        indexed_text = torch.LongTensor(word_index_tranformer.words2index_transform(cuted_text, max_len=max_sentece_length))\n",
    "        if(self.train):\n",
    "            label = label2num_dict[self.data_list[index][\"label\"]]\n",
    "            return label, indexed_text\n",
    "        else:\n",
    "            return indexed_text\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data_list)\n",
    "\n",
    "train_dataset = RosDataset(train_data_path)\n",
    "dev_dataset = RosDataset(dev_data_path)\n",
    "test_dataset = RosDataset(test_data_path, train=False)\n",
    "print(train_dataset[0])\n",
    "print(len(train_dataset))\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:tensor([[ 599, 1345,    0, 1208,    4,  344,   99,  617,   60,   95, 1230,  283,\n           95,    5,    6,    1,    1,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "max_sentece_length = 20\n",
    "def collate_fn(batch):\n",
    "    #  batch是一个列表，其中是一个一个的元组，每个元组是dataset中_getitem__的结果\n",
    "    word_index_tranformer = pickle.load(open(r\"models/KUAKE-QIC_Word2Sequence.pkl\", 'rb'))\n",
    "    batch = list(zip(*batch))\n",
    "    return torch.LongTensor(batch[0]), torch.stack(batch[1])\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset,batch_size=128,shuffle=True,collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset,batch_size=1,shuffle=True,collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, drop_last=False)\n",
    "for index, text in enumerate(test_data_loader):\n",
    "    if(index > 0):\n",
    "        break\n",
    "    print(f\"{index}:{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def train(epochs, model, model_path=None, optimizer_path=None, device=None):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in tqdm(range(epochs), desc=\"Train\"):\n",
    "        for index, (label, text) in enumerate(train_data_loader):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text = text.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if not model_path is None:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    if not optimizer_path is None:\n",
    "        torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "\n",
    "def evaluation_accuracy(model, test_data_loader, device=None):\n",
    "    count_correct = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(dev_data_loader, desc=\"Evaluation\"):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text = text.to(device)\n",
    "            if(model(text).argmax() == label):\n",
    "                count_correct = count_correct + 1\n",
    "    print(f\"\\n{count_correct}/{len(test_data_loader)}\")\n",
    "    return count_correct / len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100%|██████████| 40/40 [01:07<00:00,  1.69s/it]\n",
      "Evaluation: 100%|██████████| 1955/1955 [00:13<00:00, 146.40it/s]\n",
      "1367/1955\n",
      "\n",
      "0.69923273657289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lstm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(word_index_tranformer), 300)\n",
    "        self.bilstm = nn.LSTM(input_size=300, num_layers=3, hidden_size=128, batch_first=True,bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(128*2, 11)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input)\n",
    "        # x:[batch_size, max_len, 2*hidden_size]\n",
    "        # h_n:[2*2, batch_size, hidden_size]\n",
    "        # c_n:[2*2, batch_size, hidden_size]\n",
    "        x, (h_n, c_n) = self.bilstm(x)\n",
    "        forward = h_n[-1, :, :]\n",
    "        backward = h_n[-2, :, :]\n",
    "        # output:[batch_size, hidden_size*2]\n",
    "        output = torch.cat([forward, backward], dim=-1)\n",
    "        output = self.fc(output)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "train_mode = True\n",
    "bilstm_model = BiLSTM()\n",
    "model_path = \"models\\QIC_bilstm.pth\"\n",
    "optimizer_path = \"models\\QIC_bilstm_optim.pth\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(train_mode):\n",
    "    train(epochs=40, model=bilstm_model, model_path=\"models\\QIC_bilstm.pth\", optimizer_path=optimizer_path, device=device)\n",
    "else:\n",
    "    bilstm_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "print('\\n' + str(evaluation_accuracy(bilstm_model, dev_data_loader, device=device)))"
   ]
  },
  {
   "source": [
    "## BiLSTM model\n",
    "- embedding\n",
    "- BiLSTM\n",
    "- FC\n",
    "## LSTM Structure\n",
    "![LSTM](resrc\\LSTM.png)\n",
    "## BiRNN Structure\n",
    "![BiRNN](resrc\\BiRNN.png)\n",
    "## SRNN Structure\n",
    "![SRNN](resrc\\SRNN.png)\n",
    "## 说明\n",
    "- 模型非常的简单，只是做了embedding之后送到3层双向LSTM中后经过全连接层分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 1994/1994 [00:14<00:00, 133.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dump_file_path = \"result\\KUAKE-QIC_test.json\"\n",
    "with open(test_data_path,'r',encoding=\"utf-8\") as source:\n",
    "    data = json.load(source)\n",
    "    bilstm_model = bilstm_model.to(device)\n",
    "    bilstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, text in tqdm(enumerate(test_data_loader), desc=\"Evaluation\", total=len(test_data_loader)):\n",
    "            if not device is None:\n",
    "                text = text.to(device)\n",
    "            data[index][\"label\"] = num2label_dict[bilstm_model(text).argmax().item()]\n",
    "            json_result = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "with open(dump_file_path,'w',encoding=\"utf-8\") as destination:\n",
    "    destination.write(json_result)"
   ]
  }
 ]
}