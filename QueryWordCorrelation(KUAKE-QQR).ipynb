{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "f82ab70f6e4e0673e87f3f6960512c9807b8b91e8473aea7edbc34f463490a2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.任务描述\n",
    "查询词之间的相关性是评估两个Query所表述主题的匹配程度，即判断Query-A和Query-B是否发生转义，以及转义的程度。Query即搜索词，包括用户在搜索框中输入的词、数字、符号等内容，Query的主题是指query的专注点，用户在输入query是希望找到与query主题相关的网页。判定两个查询词之间的相关性是一项重要的任务，常用于长尾query的搜索质量优化场景，本任务数据集就是在这样的背景下产生的。\n",
    "\n",
    "## 2.任务说明\n",
    "Query和Title的相关度共分为3档（0-2），0分为相关性最差，2分表示相关性最好。\n",
    "\n",
    "2分：表示A与B等价，表述完全一致。\n",
    "\n",
    "1分： B为A的语义子集，B指代范围小于A。\n",
    "\n",
    "0分：B为A的语义父集，B指代范围大于A； 或者A与B语义毫无关联。\n",
    "\n",
    "## 3.评测指标\n",
    "本任务的评价指标使用准确率Accuracy来评估，即：\n",
    "准确率(Accuracy) = #预测正确的条目数 / #预测总条目数\n",
    "\n",
    "## 4.评测数据\n",
    "本评测开放训练集数据15000条，验证集数据1600条，测试集数据1596条。\n",
    "\n",
    "## 说明\n",
    "处理过程同KUAKE-QTR完全类似，不加赘述"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut by word\n",
    "\n",
    "def tokenize(text):\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to sequence\n",
    "UNK_TAG = \"UNK\"\n",
    "PAD_TAG = \"PAD\"\n",
    "class Word2Sequence():\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2index_dict = {\n",
    "            UNK_TAG : self.UNK,\n",
    "            PAD_TAG : self.PAD,\n",
    "        }\n",
    "        self.count = {}\n",
    "\n",
    "\n",
    "    def fit(self, sentence):\n",
    "        # 保存句子到dict, 统计词频\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "    def build_vocab(self,min=0,max=None,max_features=None):\n",
    "        self.count = {word:value for word,value in self.count.items() if value > min}\n",
    "        if(max is not None):\n",
    "            self.count = {word:value for word,value in self.count.items() if value < max}\n",
    "        if max_features is not None:\n",
    "            self.count = dict(sorted(self.count.items(), key = lambda x:x[-1], reverse=True)[:max_features])\n",
    "\n",
    "        for word in self.count:\n",
    "            self.word2index_dict[word] = len(self.word2index_dict)\n",
    "        self.index2word_dict = dict(zip(self.word2index_dict.values(), self.word2index_dict.keys()))\n",
    "\n",
    "\n",
    "    def words2index_transform(self, sentence, max_len=None):\n",
    "        if max_len is not None:\n",
    "            if max_len > len(sentence):\n",
    "                sentence = sentence + [PAD_TAG] * (max_len - len(sentence))\n",
    "            else:\n",
    "                sentence = sentence[:max_len]\n",
    "        return [self.word2index_dict.get(word, self.UNK) for word in sentence]\n",
    "\n",
    "\n",
    "    def index2words_transform(self, sentence):\n",
    "        return [self.index2word_dict.get(index) for index in sentence]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 130441.80it/s]\n",
      "2047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dictionary build\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_data_path = r\"data\\KUAKE-QQR\\KUAKE-QQR_train.json\"\n",
    "test_data_path = r\"data\\KUAKE-QQR\\KUAKE-QQR_test.json\"\n",
    "dev_data_path = r\"data\\KUAKE-QQR\\KUAKE-QQR_dev.json\"\n",
    "if(not os.path.exists(\"models/KUAKE-QQR_Word2Sequence.pkl\")):\n",
    "    word_index_tranformer = Word2Sequence()\n",
    "    with open(train_data_path, encoding=\"utf-8\") as f:\n",
    "        for data in tqdm(json.load(f)):\n",
    "            word_index_tranformer.fit(tokenize(data['query1']))\n",
    "            word_index_tranformer.fit(tokenize(data['query2']))\n",
    "    word_index_tranformer.build_vocab()\n",
    "    pickle.dump(word_index_tranformer, open(r\"models/KUAKE-QQR_Word2Sequence.pkl\", 'wb'))\n",
    "else:\n",
    "    word_index_tranformer = pickle.load(open(r\"models/KUAKE-QQR_Word2Sequence.pkl\", 'rb'))\n",
    "print('\\n' + str(len(word_index_tranformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, tensor([2, 3, 4, 5, 6, 1, 1, 1, 1, 1, 1, 1]), tensor([4, 5, 3, 7, 1, 1, 1, 1, 1, 1, 1, 1])) 15000\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "max_sentece_length = 12\n",
    "class RosDataset(Dataset):\n",
    "    def __init__(self, data_path, train=True):\n",
    "        self.train = train\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            self.data_list = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        cuted_text1 = tokenize(self.data_list[index][\"query1\"])\n",
    "        cuted_text2 = tokenize(self.data_list[index][\"query2\"])\n",
    "        indexed_text1 = torch.LongTensor(word_index_tranformer.words2index_transform(cuted_text1, max_len=max_sentece_length))\n",
    "        indexed_text2 = torch.LongTensor(word_index_tranformer.words2index_transform(cuted_text2, max_len=max_sentece_length))\n",
    "        if(self.train):\n",
    "            label = int(self.data_list[index][\"label\"])\n",
    "            return label, indexed_text1, indexed_text2\n",
    "        else:\n",
    "            return indexed_text1, indexed_text2\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data_list)\n",
    "\n",
    "train_dataset = RosDataset(train_data_path)\n",
    "dev_dataset = RosDataset(dev_data_path)\n",
    "test_dataset = RosDataset(test_data_path, train=False)\n",
    "print(train_dataset[0], len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:tensor([1, 1, 2, 1, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 2, 2, 0, 1, 2, 0, 0, 2, 0,\n        0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n        0, 0, 2, 0, 2, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0,\n        2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n        2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        2, 0, 0, 0, 0, 0, 0, 1]),tensor([[ 847,  717,  537,  ...,    1,    1,    1],\n        [ 148,  149,  150,  ...,    1,    1,    1],\n        [  67,   43,  654,  ...,  735,  517,    1],\n        ...,\n        [1454,    2,  165,  ...,  133,  462,    1],\n        [ 238,   81,  239,  ...,  159,    1,    1],\n        [ 421, 1163,   14,  ...,    1,    1,    1]]),tensor([[847, 717, 537,  ...,   1,   1,   1],\n        [148, 149, 150,  ...,   1,   1,   1],\n        [ 67,  43, 430,  ..., 735, 517,   1],\n        ...,\n        [690,  28, 974,  ..., 128, 109, 129],\n        [260, 261, 242,  ...,   1,   1,   1],\n        [421, 305, 429,  ...,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset,batch_size=128,shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset,batch_size=1,shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, drop_last=False)\n",
    "for index, (label, indexed_text1, indexed_text2) in enumerate(train_data_loader):\n",
    "    if(index > 0):\n",
    "        break\n",
    "    print(f\"{index}:{label},{indexed_text1},{indexed_text2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word_index_tranformer),embedding_dim=300,padding_idx=word_index_tranformer.PAD)\n",
    "        self.gru1 = nn.GRU(input_size=300,hidden_size=256,num_layers=2,batch_first=True,bidirectional=True)\n",
    "        self.gru2 = nn.GRU(input_size=256*4,hidden_size=256,num_layers=1,batch_first=True,bidirectional=False)\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(256*4,256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256,256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        mask1 = input1.eq(word_index_tranformer.PAD)\n",
    "        mask2 = input2.eq(word_index_tranformer.PAD)\n",
    "        input1 = self.embedding(input1)\n",
    "        input2 = self.embedding(input2)\n",
    "        output1,_ = self.gru1(input1)\n",
    "        output2,_ = self.gru1(input2)\n",
    "        \n",
    "        output1_align, output2_align = self.soft_attention_align(output1, output2, mask1, mask2)\n",
    "        output1 = torch.cat([output1, output1_align], 2)\n",
    "        output2 = torch.cat([output2, output2_align], 2)\n",
    "        \n",
    "        gru2_output1,_ = self.gru2(output1)\n",
    "        gru2_output2,_ = self.gru2(output2)\n",
    "        \n",
    "        output1_pooled = self.apply_pooling(gru2_output1)\n",
    "        output2_pooled = self.apply_pooling(gru2_output2)\n",
    "        out = torch.cat([output1_pooled, output2_pooled], dim=-1)\n",
    "        out = self.dnn(out)\n",
    "        \n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "    def apply_pooling(self, output):\n",
    "        avg_pooled = F.avg_pool1d(output.transpose(1,2), kernel_size=output.size(1)).squeeze(-1)\n",
    "        max_pooled = F.max_pool1d(output.transpose(1,2), kernel_size=output.size(1)).squeeze(-1)\n",
    "        return torch.cat([avg_pooled, max_pooled], dim=-1)\n",
    "\n",
    "\n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float(\"-inf\"))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float(\"-inf\"))\n",
    "\n",
    "        attention_weight = x1.bmm(x2.transpose(1, 2))\n",
    "        x1_weight = F.softmax(attention_weight + mask2.unsqueeze(1), dim=-1) \n",
    "        x2_output = x1_weight.bmm(x2)\n",
    "\n",
    "        x2_weight = F.softmax(attention_weight.transpose(1, 2) + mask1.unsqueeze(1), dim=-1) \n",
    "        x1_output = x2_weight.bmm(x1)\n",
    "        \n",
    "        return x1_output, x2_output"
   ]
  },
  {
   "source": [
    "## 说明\n",
    "- 可见CHIP-STS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def train(epochs, model, model_path=None, optimizer_path=None, device=None):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in tqdm(range(epochs), desc=\"Train\"):\n",
    "        for index, (label, text1, text2) in enumerate(train_data_loader):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text1, text2)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if not model_path is None:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    if not optimizer_path is None:\n",
    "        torch.save(optimizer.state_dict(), optimizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_accuracy(model, test_data_loader, device=None):\n",
    "    count_correct = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for label, text1, text2 in tqdm(dev_data_loader, desc=\"Evaluation\"):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            if(model(text1, text2).argmax() == label):\n",
    "                count_correct = count_correct + 1\n",
    "    print(f\"\\n{count_correct}/{len(test_data_loader)}\")\n",
    "    return count_correct / len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 1600/1600 [00:16<00:00, 98.61it/s] \n",
      "1080/1600\n",
      "\n",
      "0.675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mode = False\n",
    "model = SiameseNetwork()\n",
    "model_path = \"models\\QQR_SiameseNetwork.pth\"\n",
    "optimizer_path = \"models\\QQR_SiameseNetwork_optim.pth\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(train_mode):\n",
    "    train(epochs=35, model=model, model_path=model_path, optimizer_path=optimizer_path, device=device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "print('\\n' + str(evaluation_accuracy(model, dev_data_loader, device=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 1596/1596 [00:17<00:00, 93.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dump_file_path = \"result\\KUAKE-QQR_test.json\"\n",
    "with open(test_data_path,'r',encoding=\"utf-8\") as source:\n",
    "    data = json.load(source)\n",
    "    new_data = list()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, (text1, text2) in tqdm(enumerate(test_data_loader), desc=\"Evaluation\", total=len(test_data_loader)):\n",
    "            if not device is None:\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            t_dict = dict()\n",
    "            t_dict[\"id\"] = data[index][\"id\"]\n",
    "            t_dict[\"query\"] = data[index][\"query1\"]\n",
    "            t_dict[\"title\"] = data[index][\"query2\"]\n",
    "            t_dict[\"label\"] = str(model(text1, text2).argmax().item())\n",
    "            new_data.append(t_dict)\n",
    "            json_result = json.dumps(new_data, ensure_ascii=False)\n",
    "\n",
    "with open(dump_file_path,'w',encoding=\"utf-8\") as destination:\n",
    "    destination.write(json_result)"
   ]
  }
 ]
}