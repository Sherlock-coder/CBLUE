{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "f82ab70f6e4e0673e87f3f6960512c9807b8b91e8473aea7edbc34f463490a2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.任务描述\n",
    "临床试验是指通过人体志愿者也称为受试者进行的科学研究，目的是确定一种药物或一项治疗方法的疗效、安全性以及存在的副作用，对于促进医学发展和提高人类健康都起到关键的作用。根据试验目的等不同，受试者可能是患者或健康志愿者。筛选标准是临床试验负责人拟定的鉴定受试者是否满足某项临床试验的主要指标，分为入组标准和排出标准，一般为无规则的自由文本形式。临床试验的受试者招募一般是通过人工比较病历记录表和临床试验筛选标准完成，这种方式费时费力且效率低下。因此，临床试验面临诸多困境，比如受试者招募困难导致临床试验难以按期完成，入组患者流失影响试验的有效性等。近年来，随着临床试验设计越来越负责，数目越来越多，基于自然语言处理和信息抽取的系统也开始在临床试验受试者招募中崭露头角并表现出不错的效果，且具有很大的实际应用前景和医学临床价值。目前这类研究大多集中在英文临床试验筛选标准及英文电子健康记录数据，针对中文电子健康数据的研究也以及取得了很多进展，然而与中文临床试验筛选标准的自然语言处理研究很少。本任务就是在这样的背景下产生的，并在CHIP2019会议发布了评测任务(http://cips-chip.org.cn/)。\n",
    "本次评测任务的主要目标是针对临床试验筛选标准进行分类，所有文本数据均来自于真实临床试验，短文本数据来源于中文临床试验注册网站(http://chictr.org.cn/)的临床试验公示信息中的筛选标准模块。数据公开透明，官网也提供下载链接。\n",
    "\n",
    "## 2.任务说明\n",
    "在本次评测中，我们给定事先定义好的44种筛选标准语义类别(详见附件的category.xlsx)和一系列中文临床试验筛选标准的描述句子，参赛者需返回每一条筛选标准的具体类别。\n",
    "标注数据示例如下：\n",
    "- S1 年龄>80岁 Age\n",
    "- S2 近期颅内或椎管内手术史 Therapy or Surgery\n",
    "- S3 血糖<2.7mmol/L Laboratory Examinations\n",
    "## 3.评测指标\n",
    "本任务的评价指标使用宏观F1值(Macro-F1，或称Average-F1)。最终排名以Macro-F1值为基准。假设我们有n个类别，C1, … …, Ci, … …, Cn。\n",
    "- 准确率Pi = 正确预测为类别Ci的样本个数 / 预测为Ci类的样本个数。\n",
    "- 召回率Ri = 正确预测为类别Ci的样本个数 / 真实的Ci类的样本个数。\n",
    "- $Macro-F1 = (1/n)\\sum_{i=1}^n{\\frac {2*Pi*Ri} {Pi+Ri}}$\n",
    "## 4.评测数据\n",
    "本评测开放训练集数据22962条，验证集数据7682条，测试集数据10000条(注：leaderboard的测试数据和原CHIP评测任务的测试数据集不是同一份，重新标注了10000条数据集)。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    # 训练数据集\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            self.data_list = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        text = self.data_list[index][\"text\"]\n",
    "        label = self.data_list[index][\"label\"]\n",
    "        return (label, text)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data_list)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    # 测试数据集\n",
    "    def __init__(self, data_path):\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            self.data_list = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        text = self.data_list[index][\"text\"]\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data_list)\n",
    "\n",
    "train_data_path = r\"data\\CHIP-CTC\\CHIP-CTC_train.json\"\n",
    "test_data_path = r\"data\\CHIP-CTC\\CHIP-CTC_test.json\"\n",
    "dev_data_path = r\"data\\CHIP-CTC\\CHIP-CTC_dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ROSENB~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.813 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "# data preprocess\n",
    "# 为了符合faxttext的使用要求\n",
    "def text_preprocess(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "\n",
    "def label_preprocess(label):\n",
    "    return \"__label__\" + label.replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "def label_depreprocess(label):\n",
    "    return label[len(\"__label__\"):].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "train_dataset_loader = DataLoader(dataset=TrainDataset(train_data_path), batch_size=1, shuffle=True, drop_last=True)\n",
    "output_file_path = \"models\\CTC_fasttext_data.txt\"\n",
    "with open(output_file_path,'w',encoding=\"utf-8\") as file:\n",
    "    for label, text in train_dataset_loader:\n",
    "        line_formatted = text_preprocess(text[0]) + '\\t' + label_preprocess(label[0])\n",
    "        file.write(line_formatted + '\\n')\n",
    "dev_dataset_loader = DataLoader(dataset=TrainDataset(dev_data_path), batch_size=1, shuffle=False, drop_last=True)\n",
    "test_dataset_loader = DataLoader(dataset=TestDataset(test_data_path), batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Wall time: 47.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fasttext\n",
    "import fasttext\n",
    "\n",
    "model_path = \"models\\CTC_fasttext.model\"\n",
    "def build_classify_model():\n",
    "    # 训练分类模型\n",
    "    model = fasttext.train_supervised(output_file_path, epoch=140, wordNgrams=5, minCount=3)\n",
    "    model.save_model(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_classify_model():\n",
    "    # 获得分类模型\n",
    "    model = fasttext.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "    \n",
    "train = True\n",
    "if(train):\n",
    "    model = build_classify_model()\n",
    "model = get_classify_model()\n",
    "    "
   ]
  },
  {
   "source": [
    "## fasttext\n",
    "![fasttext](resrc\\faxttext.png)\n",
    "\n",
    "[Bag of Tricks for Efficient Text Classification](file:///D:/Files/NLP%20Project/works/TianChi/paper/CHC/Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification.pdf)\n",
    "- The features are embedded and averaged to form the hidden variable\n",
    "- A bag of n-grams as additional features to capture some partial information about the local word.\n",
    "- In order to improve running time, using a hierarchical softmax based on the Huffman coding tree\n",
    "## 实验\n",
    "- 有趣的是，只有在训练到140左右这个比较不合理的epoch数之后模型在验证集上的表现才达到一个比较好的效果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6360891888929809\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "label_name=\"\"\"Disease\n",
    "Symptom\n",
    "Sign\n",
    "Pregnancy-related Activity\n",
    "Neoplasm Status\n",
    "Non-Neoplasm Disease Stage\n",
    "Allergy Intolerance\n",
    "Organ or Tissue Status\n",
    "Life Expectancy\n",
    "Oral related\n",
    "Pharmaceutical Substance or Drug\n",
    "Therapy or Surgery\n",
    "Device\n",
    "Nursing\n",
    "Diagnostic\n",
    "Laboratory Examinations\n",
    "Risk Assessment\n",
    "Receptor Status\n",
    "Age\n",
    "Special Patient Characteristic\n",
    "Literacy\n",
    "Gender\n",
    "Education\n",
    "Address\n",
    "Ethnicity\n",
    "Consent\n",
    "Enrollment in other studies\n",
    "Researcher Decision\n",
    "Capacity\n",
    "Ethical Audit\n",
    "Compliance with Protocol\n",
    "Addictive Behavior\n",
    "Bedtime\n",
    "Exercise\n",
    "Diet\n",
    "Alcohol Consumer\n",
    "Sexual related\n",
    "Smoking Status\n",
    "Blood Donation\n",
    "Encounter\n",
    "Disabilities\n",
    "Healthy\n",
    "Data Accessible\n",
    "Multiple\"\"\"\n",
    "\n",
    "\n",
    "def evaluation_macro_f1(model, test_lines, test_labels):\n",
    "    # 人工实现的maro_f1计算\n",
    "    prediction = np.array(model.predict(test_lines)[0])\n",
    "    prediction = prediction.reshape(prediction.shape[0])\n",
    "    accurate = (prediction == test_labels)\n",
    "    label_names = [label_preprocess(line) for line in label_name.split('\\n')] \n",
    "    count_prediction_label = np.zeros(len(label_names))\n",
    "    count_prediction_accurate_label = np.zeros(len(label_names))\n",
    "    count_total_label = np.zeros(len(label_names))\n",
    "    for i in range(prediction.shape[0]):\n",
    "        for j in range(len(label_names)):\n",
    "            if(prediction[i] == label_names[j]):\n",
    "                count_prediction_label[j] = count_prediction_label[j] + 1\n",
    "                if(accurate[i]):\n",
    "                    count_prediction_accurate_label[j] = count_prediction_accurate_label[j] + 1\n",
    "    for i in range(len(test_labels)):\n",
    "        for j in range(len(label_names)):\n",
    "            if(test_labels[i] == label_names[j]):\n",
    "                count_total_label[j] = count_total_label[j] + 1\n",
    "    P = np.nan_to_num((count_prediction_accurate_label / count_prediction_label))\n",
    "    R = np.nan_to_num((count_prediction_accurate_label /  count_total_label))\n",
    "    result = np.nan_to_num((2 * P * R)/(P + R))\n",
    "    return result.mean()\n",
    "\n",
    "test_lines = list()\n",
    "test_labels = list()\n",
    "for label, text in dev_dataset_loader:\n",
    "    test_lines.append(text_preprocess(text[0]))\n",
    "    test_labels.append(label_preprocess(label[0]))\n",
    "print(evaluation_macro_f1(model, test_lines, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump to json\n",
    "dump_file_path = \"result\\CHIP-CTC_test.json\"\n",
    "with open(test_data_path,'r',encoding=\"utf-8\") as source:\n",
    "    texts = list()\n",
    "    data = json.load(source)\n",
    "    for text in test_dataset_loader:\n",
    "        texts.append(text_preprocess(text[0]))\n",
    "    \n",
    "    prediction = np.array(model.predict(texts)[0])\n",
    "    prediction = prediction.reshape(prediction.shape[0])\n",
    "    \n",
    "    for index in range(len(data)):\n",
    "        data[index][\"label\"] = label_depreprocess(prediction[index])\n",
    "    json_result = json.dumps(data, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "with open(dump_file_path,'w',encoding=\"utf-8\") as destination:\n",
    "    destination.write(json_result)"
   ]
  }
 ]
}