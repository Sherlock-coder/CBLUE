{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "f82ab70f6e4e0673e87f3f6960512c9807b8b91e8473aea7edbc34f463490a2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.任务描述\n",
    "临床术语标准化任务是医学统计中不可或缺的一项任务。临床上，关于同一种诊断、手术、药品、检查、化验、症状等往往会有成百上千种不同的写法。标准化（归一）要解决的问题就是为临床上各种不同说法找到对应的标准说法。有了术语标准化的基础，研究人员才可对电子病历进行后续的统计分析。本质上，临床术语标准化任务也是语义相似度匹配任务的一种。但是由于原词表述方式过于多样，单一的匹配模型很难获得很好的效果。本任务就是在这样的背景下产生的，并在CHIP2020会议发布了评测任务(http://cips-chip.org.cn/)。\n",
    "\n",
    "## 2.任务说明\n",
    "本次评测任务主要目标是针对中文电子病历中挖掘出的真实诊断实体进行语义标准化。 给定一诊断原词，要求给出其对应的诊断标准词。所有诊断原词均来自于真实医疗数据，并以《国际疾病分类 ICD-10 北京临床版v601》词表为标准进行了标注。标注样例如下（注：预测值可能存在多个，用“##”分隔）：\n",
    "\n",
    "- 右肺结节转移可能大 <-> 肺占位性病变##肺继发恶性肿瘤##转移性肿瘤\n",
    "- 右肺结节住院 <-> 肺占位性病变\n",
    "- 左上肺胸膜下结节待查 <-> 胸膜占位\n",
    "## 3.评测指标\n",
    "以(诊断原词，标准词)作为基本单位计算F1得分。如测试集有m对(诊断原词，标准词)组合，预测了n对(诊断原词，标准词)组合，有k对组合是预测正确的。\n",
    "$P = k / n$\n",
    "\n",
    "$R = k / m$\n",
    "\n",
    "$F1 = 2 * P * R / (P+R)$\n",
    "\n",
    "## 4.评测数据\n",
    "本评测开放训练集数据6000条，验证集数据2000条（注：原CHIP评测中只提供了8000条训练集，依数据方专家建议，本leaderboard中切分为训练集、验证集分别为6000和2000条，验证集不可用做训练），测试集数据10000条。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0           霍乱,由于01群霍乱弧菌,霍乱生物型所致\n1                        古典生物型霍乱\n2          霍乱,由于01群霍乱弧菌,埃尔托生物型所致\n3                       埃尔托生物型霍乱\n4                         未特指的霍乱\n                  ...           \n40468      与烷化剂有关与治疗有关的骨髓增生异常综合征\n40469    与表鬼臼毒素有关与治疗有关的骨髓增生异常综合征\n40470                  骨髓增生异常综合征\n40471                      白血病前期\n40472                   白血病前期综合征\nName: 霍乱, Length: 40473, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# data read-in\n",
    "import pandas as pd\n",
    "\n",
    "#从excel中读入匹配词集\n",
    "terminology_table = pd.read_excel(r\"data\\CHIP-CDN\\国际疾病分类 ICD-10北京临床版v601.xlsx\")\n",
    "terminologies = terminology_table[\"霍乱\"]\n",
    "print(terminologies)"
   ]
  },
  {
   "source": [
    "## 匹配词说明\n",
    "- 可以注意到这里的待匹配词有40000多个\n",
    "    - 在完全不考虑时间成本的情况当然可以使用DNN的方法逐一匹配\n",
    "    - 但显然这是不太现实的，而且在逐一匹配的情况下显然是无法使用接下来用的二分类孪生神经网络\n",
    "        - 假设确实使用二分类方法逐一匹配，完全可以预测相当多的词网络会给出匹配的判断，这显然是不合理的\n",
    "        - 这一点事实实际上已经点出了接下来处理流程的固有缺陷，将在模型部分具体说明\n",
    "- 其次要注意的是这些术语其实不是完全无组织的\n",
    "    - 在大类上原术语分为27类，这也是下面在使用Cluster pruning召回时, k_cluster参数特设为27的理由\n",
    "        - 但事实上，由于在cluster pruning时cluster的leader时随机选定的，并不能说27个cluster确实一一对应了原来的大类\n",
    "        - 但经目测，在特设这一次参数后，召回效果还是有肉眼可见的提升的\n",
    "            - 但这依旧不能作为利用到了术语分类的固有属性的佐证\n",
    "            - 因为减少k_cluster其实也完全可能是因为是事实上增大了搜索范围而导致的效果提升\n",
    "    - 在小类和次小类的上术语仍有细分\n",
    "    - 由于上述的说明，完全可以得到当前面临如此大匹配词集时可能的解决方案\n",
    "        - 类似层次化softmax的多级分类\n",
    "        - 利用预设条件来做无监督分类后再将代匹配词映射到语义空间后做召回\n",
    "    - 上述思路将在模型部分做详细说明"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut by word\n",
    "def tokenize(text):\n",
    "    result = list(text)\n",
    "    return result"
   ]
  },
  {
   "source": [
    "## 关于这里分词方法的reflection可见KUAKE-QIC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['不稳定性心绞痛', '稳定性心绞痛', '糖尿病性高血压', '糖尿病性低血糖症', '糖尿病性心肌病', '糖尿病性缺血性心肌病', '糖尿病性体位性低血压', '1型糖尿病性高血压', '2型糖尿病性高血压', '2型糖尿病性低血糖症']\nWall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Recall\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pysparnn.cluster_index as ci\n",
    "import pickle\n",
    "\n",
    "\n",
    "recall_search_index_path = r\"models\\recall_search.index\"\n",
    "class Sentence2Vector:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build_vectors(self, sentences):\n",
    "        lines_cuted = [\" \".join(tokenize(sentence)) for sentence in sentences]\n",
    "        # tfidf 根据词频和文档的词语\n",
    "        vertorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        feature_vec = vertorizer.fit_transform(lines_cuted)\n",
    "        search_index = self.get_cp(feature_vec, sentences)\n",
    "        return vertorizer, feature_vec, lines_cuted, search_index\n",
    "\n",
    "    def build_cp(self, vectors, data):\n",
    "        search_index = ci.MultiClusterIndex(vectors, data)\n",
    "        pickle.dump(search_index, open(recall_search_index_path, \"wb\"))\n",
    "        return search_index\n",
    "\n",
    "    def get_cp(self, vectors, data):\n",
    "        if(os.path.exists(recall_search_index_path)):\n",
    "            search_index = pickle.load(open(recall_search_index_path, \"rb\"))\n",
    "        else:\n",
    "            search_index = self.build_cp(vectors, data)\n",
    "        return search_index\n",
    "\n",
    "class Recall:\n",
    "    def __init__(self, sentences, k=10):\n",
    "        self.k = k\n",
    "        sentence_vec = Sentence2Vector()\n",
    "        self.vertorizer, self.feature_vec, self.lines_cuted, self.search_index = sentence_vec.build_vectors(sentences)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        sentence_vector = self.vertorizer.transform(sentence)\n",
    "        return self.search_index.search(sentence_vector, k=self.k, k_clusters=27, return_distance=False)\n",
    "\n",
    "print(Recall(terminologies).predict([\" \".join(tokenize(\"糖尿病反复低血糖;骨质疏松;高血压冠心病不稳定心绞痛\"))])[0])"
   ]
  },
  {
   "source": [
    "## Recall\n",
    "#### For more 可见《[信息检索导论](file:///D:/Files/NLP%20Project/material/More/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA.pdf)》第六章\n",
    "### TfidfVectorizer\n",
    "- 词项频率(term frequencey)记为 $tf_{t,d}$，其中的两个下标分别对应词项和文档, 它表示t在文档中的出现次数\n",
    "- 文档频率(document frequency)${df}_t$，它表示的是出现t的所有文档的数目\n",
    "    -  词项t的idf(inverse document frequency, 逆文档频率)的定义如下:$idf_t = \\log{\\frac{N}{df_t}}$\n",
    "        - N为所有文档的数目\n",
    "- tf-idf 权重机制对文档d中的词项t赋予的权重如下:$tf-idf_{t,d} = tf_{t,d} × idf_t$\n",
    "- tf-idf vectorization:文档看成是一个vector，其中的每个分量都对应词典中的一个词项，分量值为采用上述公式计算出的权重值\n",
    "- 需要强调的是，计算的公式方法是不唯一的和且有许许多多改进，重要的是它的语义\n",
    "- 这种向量化的方式可能导致的问题已经在前述提过\n",
    "\n",
    "### 向量空间模型(vector space model，简称 VSM)\n",
    "- 为了弥补文档长度给上述相似度计算所带来的负面效果，计算两篇文档$d_1$和$d_2$相似度的常规方法是求余弦相似度(cosine similarity):$sim(d_1, d_2)=\\frac{V(d_1)V(d_2)}{|V(d_1)||V(d_2)|}$，其中V(d)表示对文档d向量化，特别的，在这里我们使用的向量化就是上述td-idf vectorization\n",
    "\n",
    "### [Cluster pruning](https://nlp.stanford.edu/IR-book/html/htmledition/cluster-pruning-1.html)\n",
    "In cluster pruning we have a preprocessing step during which we cluster the document vectors. Then at query time, we consider only documents in a small number of clusters as candidates for which we compute cosine scores.\n",
    "\n",
    "1. Pick $\\sqrt{N}$ documents at random from the collection. Call these leaders.\n",
    "2. For each document that is not a leader, we compute its nearest leader.\n",
    "\n",
    "We refer to documents that are not leaders as followers. Intuitively, in the partition of the followers induced by the use of $\\sqrt{N}$ randomly chosen leaders, the expected number of followers for each leader is $\\approx N/\\sqrt{N} = \\sqrt{N}$. Next, query processing proceeds as follows:\n",
    "\n",
    "1. Given a query $q$, find the leader $L$ that is closest to $q$. This entails computing cosine similarities from $q$ to each of the $\\sqrt{N}$ leaders.\n",
    "2. The candidate set $A$ consists of $L$ together with its followers. We compute the cosine scores for all documents in this candidate set.\n",
    "\n",
    "The use of randomly chosen leaders for clustering is fast and likely to reflect the distribution of the document vectors in the vector space: a region of the vector space that is dense in documents is likely to produce multiple leaders and thus a finer partition into sub-regions. \n",
    "\n",
    "![Cluster pruning](resrc\\cluster_pruning.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to sequence\n",
    "UNK_TAG = \"UNK\"\n",
    "PAD_TAG = \"PAD\"\n",
    "class Word2Sequence():\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2index_dict = {\n",
    "            UNK_TAG : self.UNK,\n",
    "            PAD_TAG : self.PAD,\n",
    "        }\n",
    "        self.count = {}\n",
    "\n",
    "\n",
    "    def fit(self, sentence):\n",
    "        # 保存句子到dict, 统计词频\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "    def build_vocab(self,min=0,max=None,max_features=None):\n",
    "        self.count = {word:value for word,value in self.count.items() if value > min}\n",
    "        if(max is not None):\n",
    "            self.count = {word:value for word,value in self.count.items() if value < max}\n",
    "        if max_features is not None:\n",
    "            self.count = dict(sorted(self.count.items(), key = lambda x:x[-1], reverse=True)[:max_features])\n",
    "\n",
    "        for word in self.count:\n",
    "            self.word2index_dict[word] = len(self.word2index_dict)\n",
    "        self.index2word_dict = dict(zip(self.word2index_dict.values(), self.word2index_dict.keys()))\n",
    "\n",
    "\n",
    "    def words2index_transform(self, sentence, max_len=None):\n",
    "        if max_len is not None:\n",
    "            if max_len > len(sentence):\n",
    "                sentence = sentence + [PAD_TAG] * (max_len - len(sentence))\n",
    "            else:\n",
    "                sentence = sentence[:max_len]\n",
    "        return [self.word2index_dict.get(word, self.UNK) for word in sentence]\n",
    "\n",
    "\n",
    "    def index2words_transform(self, sentence):\n",
    "        return [self.index2word_dict.get(index) for index in sentence]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 146381.87it/s]\n",
      "2560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dictionary build\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_data_path = r\"data\\CHIP-CDN\\CHIP-CDN_train.json\"\n",
    "test_data_path = r\"data\\CHIP-CDN\\CHIP-CDN_test.json\"\n",
    "dev_data_path = r\"data\\CHIP-CDN\\CHIP-CDN_dev.json\"\n",
    "if(not os.path.exists(\"models/STS_Word2Sequence.pkl\")):\n",
    "    word_index_tranformer = Word2Sequence()\n",
    "    with open(train_data_path, encoding=\"utf-8\") as f:\n",
    "        for data in tqdm(json.load(f)):\n",
    "            word_index_tranformer.fit(tokenize(data['text']))\n",
    "            word_index_tranformer.fit(tokenize(data['normalized_result'].replace(\"##\",\"\")))\n",
    "    for terminology in terminologies:\n",
    "        word_index_tranformer.fit(tokenize(terminology))\n",
    "    word_index_tranformer.build_vocab()\n",
    "    pickle.dump(word_index_tranformer, open(r\"models/CHIP-CDN_Word2Sequence.pkl\", 'wb'))\n",
    "else:\n",
    "    word_index_tranformer = pickle.load(open(r\"models/CHIP-CDN_Word2Sequence.pkl\", 'rb'))\n",
    "print(\"\\n\" + str(len(word_index_tranformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "dev: 100%|██████████| 2000/2000 [00:00<00:00, 502251.71it/s]\n",
      "test: 100%|██████████| 10000/10000 [00:00<00:00, 1249420.32it/s]\n",
      "---------------------------------\n",
      "(1, tensor([2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([ 3, 10, 11, 12, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1])) 36250\n",
      "---------------------------------\n",
      "('卵巢Ca', ['卵巢恶性肿瘤', '癌']) 2000\n",
      "---------------------------------\n",
      "﻿左前降支中段肌桥－壁冠状动脉MB-MCA 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_sentece_length = 20\n",
    "class RosDataset(Dataset):\n",
    "    def __init__(self, data_path, mode):\n",
    "        # 使用recall的结果来构造负例\n",
    "        # mode: \n",
    "        # 0->train\n",
    "        # 1->dev\n",
    "        # 2->test\n",
    "\n",
    "        self.mode = mode\n",
    "        self.data = list()\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            data_list = json.load(f)\n",
    "            data_path = r\"models\\CHIP-CDN_trainData\"\n",
    "            if(self.mode == 0):\n",
    "                if(os.path.exists(data_path)):\n",
    "                    self.data = pickle.load(open(data_path, \"rb\"))\n",
    "                else:\n",
    "                    recall = Recall(terminologies, 5)\n",
    "                    for pair in tqdm(data_list, desc=\"train\"):\n",
    "                        text = tokenize(pair[\"text\"])\n",
    "                        matched_results = pair[\"normalized_result\"].split(\"##\")\n",
    "                        ret = recall.predict([\" \".join(text)])[0]\n",
    "                        for r in matched_results:\n",
    "                            self.data.append([text, tokenize(r), 1])\n",
    "                            if r in ret:\n",
    "                                ret.remove(r)\n",
    "                        for r in ret:\n",
    "                            self.data.append([text, tokenize(r), 0])\n",
    "                    pickle.dump(self.data, open(data_path, \"wb\"))\n",
    "            elif(self.mode == 1):\n",
    "                for pair in tqdm(data_list, desc=\"dev\"):\n",
    "                    text = pair[\"text\"]\n",
    "                    matched_results = pair[\"normalized_result\"].split(\"##\")\n",
    "                    self.data.append([text, matched_results])\n",
    "            else:\n",
    "                for pair in tqdm(data_list, desc=\"test\"):\n",
    "                    text = pair[\"text\"]\n",
    "                    self.data.append([text])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        text = self.data[index][0]\n",
    "        if(self.mode == 0):\n",
    "            indexed_text = torch.LongTensor(word_index_tranformer.words2index_transform(text, max_len=max_sentece_length))\n",
    "            indexed_match = torch.LongTensor(word_index_tranformer.words2index_transform(self.data[index][1], max_len=max_sentece_length))\n",
    "            label = self.data[index][2]\n",
    "            return label, indexed_text, indexed_match\n",
    "        elif(self.mode == 1):\n",
    "            text = self.data[index][0]\n",
    "            matches = self.data[index][1]\n",
    "            return text, matches\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = RosDataset(train_data_path, mode=0)\n",
    "dev_dataset = RosDataset(dev_data_path, mode=1)\n",
    "test_dataset = RosDataset(test_data_path, mode=2)\n",
    "print(\"\\n---------------------------------\")\n",
    "print(train_dataset[0], len(train_dataset))\n",
    "print(\"---------------------------------\")\n",
    "print(dev_dataset[1], len(dev_dataset))\n",
    "print(\"---------------------------------\")\n",
    "print(test_dataset[0], len(test_dataset))"
   ]
  },
  {
   "source": [
    "## 数据集构建说明\n",
    "- 由于我们依旧采用了相互比对并进行二分类的模型，我们就必须把原数据处理成一对对匹配对\n",
    "- 而由于训练集中只给出了每一个词及其对应的标准化结果，也即只事实上存在正例，所以我们必须构造负例\n",
    "- 所以我们最终构造的方法如下\n",
    "    - 毫无疑问的，原来词及其标准化结果全部被处理成正例加入数据集\n",
    "    - 对于每一个词，使用召回模块得到候选词，特别的，这里我们选了5个候选词\n",
    "        - 这里候选词的个数是可以选定，这里选定5个词的理由是使得正负例的比例大致为1:4\n",
    "            - 平均而言，一个词的标准结果大概为一个多词，再考虑召回的失配率，大致为1:4\n",
    "    - 对于这5个候选词，如果它是标准化后的结果，那不必处理，因为它必然会作为正例加入数据集\n",
    "    - 如果它不是标准化的结果，则将其作为负样例加入数据集\n",
    "        - 当然，我们可以在40000个候选词随机选择一定量非标准化结果词作为负样例加入数据集，但它的坏处是显而易见的\n",
    "        - 由于我们最终做预测时必然先进行召回，再对召回结果再进行匹配\n",
    "        - 所以模型训练的关键在于成功从召回结果选出正确的标准化结果\n",
    "            - 也即从已经召回的较为相似的结果中选择正确筛选\n",
    "            - 这需要拉开文档向量空间中原本已经较为接近的向量的距离，这也正是负样例的意义所在\n",
    "        - 而随机选择没有办法使得模型具有这种区分能力\n",
    "            - 在理想情况下，对于完全不相似的词，模型应该很容易获得区分他们的能力而不需要专门的训练\n",
    "            - 其次，由于召回过程已经经过初筛，即使模型在方面的能力有所欠缺也不重要"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:颅内脱髓鞘病变待诊, ['脱髓鞘病']\n1:上消化道出血(食道胃底静脉曲张), ['上消化道出血', '食管静脉曲张', '胃底静脉曲张']\n2:冠心病高血压病高脂血症, ['冠状动脉粥样硬化性心脏病', '高血压', '高脂血症']\n3:骨髓淋巴细胞增生, ['淋巴细胞增殖性疾病']\n4:颈椎椎间盘膨出, ['其他的颈椎间盘移位']\n5:子宫颈鳞癌IIB期放化疗后, ['子宫颈恶性肿瘤', '鳞状细胞癌', '恶性肿瘤放疗', '化学治疗']\n0:['\\ufeff左前降支中段肌桥－壁冠状动脉MB-MCA']\n1:['子宫脱出III阴道前壁膨出III']\n2:['右手掌撕脱伤并手内肌损伤']\n3:['右侧第11肋骨骨折']\n4:['皮肤色素沉着原因待查1Addison病']\n5:['十二指肠及空肠切除术']\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(data):\n",
    "    return data[0][0], data[0][1]\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset,batch_size=128,shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset,batch_size=1,shuffle=True,collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, drop_last=False)\n",
    "\n",
    "for index, (text, matches) in enumerate(dev_data_loader):\n",
    "    if(index > 5):\n",
    "        break\n",
    "    print(f\"{index}:{text}, {matches}\")\n",
    "\n",
    "for index, text in enumerate(test_data_loader):\n",
    "    if(index > 5):\n",
    "        break\n",
    "    print(f\"{index}:{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word_index_tranformer),embedding_dim=300,padding_idx=word_index_tranformer.PAD)\n",
    "        self.gru1 = nn.GRU(input_size=300,hidden_size=256,num_layers=2,batch_first=True,bidirectional=True)\n",
    "        self.gru2 = nn.GRU(input_size=256*4,hidden_size=256,num_layers=1,batch_first=True,bidirectional=False)\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(256*4,256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256,256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        mask1 = input1.eq(word_index_tranformer.PAD)\n",
    "        mask2 = input2.eq(word_index_tranformer.PAD)\n",
    "        input1 = self.embedding(input1)\n",
    "        input2 = self.embedding(input2)\n",
    "        output1,_ = self.gru1(input1)\n",
    "        output2,_ = self.gru1(input2)\n",
    "        \n",
    "        output1_align, output2_align = self.soft_attention_align(output1, output2, mask1, mask2)\n",
    "        output1 = torch.cat([output1, output1_align], 2)\n",
    "        output2 = torch.cat([output2, output2_align], 2)\n",
    "        \n",
    "        gru2_output1,_ = self.gru2(output1)\n",
    "        gru2_output2,_ = self.gru2(output2)\n",
    "        \n",
    "        output1_pooled = self.apply_pooling(gru2_output1)\n",
    "        output2_pooled = self.apply_pooling(gru2_output2)\n",
    "        out = torch.cat([output1_pooled, output2_pooled], dim=-1)\n",
    "        out = self.dnn(out)\n",
    "        \n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "    def apply_pooling(self, output):\n",
    "        avg_pooled = F.avg_pool1d(output.transpose(1,2), kernel_size=output.size(1)).squeeze(-1)\n",
    "        max_pooled = F.max_pool1d(output.transpose(1,2), kernel_size=output.size(1)).squeeze(-1)\n",
    "        return torch.cat([avg_pooled, max_pooled], dim=-1)\n",
    "\n",
    "\n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float(\"-inf\"))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float(\"-inf\"))\n",
    "\n",
    "        attention_weight = x1.bmm(x2.transpose(1, 2))\n",
    "        x1_weight = F.softmax(attention_weight + mask2.unsqueeze(1), dim=-1) \n",
    "        x2_output = x1_weight.bmm(x2)\n",
    "\n",
    "        x2_weight = F.softmax(attention_weight.transpose(1, 2) + mask1.unsqueeze(1), dim=-1) \n",
    "        x1_output = x2_weight.bmm(x1)\n",
    "        \n",
    "        return x1_output, x2_output"
   ]
  },
  {
   "source": [
    "## 模型说明\n",
    "模型本身是和其他任务完全类似的，不加赘述\n",
    "\n",
    "### 其他任务中没具体提到的细节\n",
    "- dropout \n",
    "    - $y=f(Wmask(x)+b)$\n",
    "    - 其中mask(x)在训练时按概率为p的伯努利分布随机生成，在测试时则将输入x乘以p做补偿\n",
    "- batch normalization\n",
    "    - 批量归一化（Batch Normalization，BN）方法是一种逐层归一化方法，对神经网络中任意的中间层进行归一化操作\n",
    "    - 公式如下：$\\hat{z}^{(l)}=\\frac{z^{(l)}-\\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}⊙\\gamma+\\beta$\n",
    "    - 其中$z^{(l)}$为前一层仿射变换后的结果，$\\mu_B$为当前batch的均值，$\\sigma^2_B$为当前batch的方差\n",
    "- soft-attention\n",
    "    - ![attention](resrc\\attention.png)\n",
    "- pooling\n",
    "- GRU\n",
    "    - ![GRU](resrc\\GRU.png)\n",
    "\n",
    "#### For more and details可参考《[神经网络与深度学习](file:///D:/Files/NLP%20Project/material/ML/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.pdf)》\n",
    "## 其他说明\n",
    "#### 需要强调的，以下内容都是想法而没有经过严格实验\n",
    "### 固有缺陷\n",
    "- 这是一个pipline的方案，显然前一阶段的误差必然会成为后一阶段的瓶颈\n",
    "- 在这里，召回的准确率就会成为整体的瓶颈\n",
    "- 其次，由于数据集的构建时经过召回初筛的，模型本身的鲁棒性值得怀疑\n",
    "### 多级分类\n",
    "- 这是对于匹配词候选集过大而提出的idea，它的想法来自hierarchical softmax\n",
    "\n",
    "![hierarchical softmax](resrc\\层次化softmax.jpg)\n",
    "- hierarchical softmax将一个多分类问题改造成一个多层的0-1 softmax，这一思路其实在这里同样使用\n",
    "- 其次由于40000个词本身是分类别的，完全可以根据这些词本身的分类别来进行多级分类\n",
    "### 无监督\n",
    "- pipline的缺陷前面已经提到了，因而不难想到训练dnn来做embedding的工具再利用cluster purning一步到位\n",
    "- 其实无论是无监督还是有监督的方法都可能做到这一点\n",
    "    - 特别的，在CHIP-CTC中使用的faxttext也可能有一定效果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def train(epochs, model, model_path=None, optimizer_path=None, device=None):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    t = tqdm(range(epochs), desc=\"Train\")\n",
    "    for epoch in t:\n",
    "        for index, (label, text1, text2) in enumerate(train_data_loader):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text1, text2)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            t.set_description(f\"epoch:{epoch}\")\n",
    "            t.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if not model_path is None:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    if not optimizer_path is None:\n",
    "        torch.save(optimizer.state_dict(), optimizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = True\n",
    "model = SiameseNetwork()\n",
    "model_path = \"models\\CHIP-CDN_SiameseNetwork.pth\"\n",
    "optimizer_path = \"models\\CHIP-CDN_SiameseNetwork_optim.pth\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(train_mode):\n",
    "    train(epochs=16, model=model, model_path=model_path, optimizer_path=optimizer_path, device=device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['肿瘤标记物升高', '乳糖不耐受']\n"
     ]
    }
   ],
   "source": [
    "class Prediction:\n",
    "    def __init__(self, model, recall, device):\n",
    "        self.device = device\n",
    "        self.recall = recall\n",
    "        self.model = model\n",
    "        model = model.to(device)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        # predict的过程为先召回选择一定数量的词再使用模型逐一匹配\n",
    "        rec = self.recall.predict([\" \".join(tokenize(sentence))])\n",
    "        result = list()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for r in rec[0]:\n",
    "                indexed_sentence = torch.LongTensor(word_index_tranformer.words2index_transform(tokenize(sentence), max_len=max_sentece_length)).to(self.device)\n",
    "                indexed_r = torch.LongTensor(word_index_tranformer.words2index_transform(tokenize(r), max_len=max_sentece_length)).to(self.device)\n",
    "                model_prediction = self.model(indexed_sentence.unsqueeze(0), indexed_r.unsqueeze(0)).argmax().item()\n",
    "                if(model_prediction == 1):\n",
    "                    result.append(r)\n",
    "        if(len(result) == 0):\n",
    "            result.append(rec[0][0])\n",
    "        return result\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "p = Prediction(model, Recall(terminologies, 5), device)\n",
    "print(p.predict(\"肿标升高待查全麻胃肠镜丙泊酚不耐受\"))"
   ]
  },
  {
   "source": [
    "## Predition的过程\n",
    "1. 使用recall模块召回候选词\n",
    "2. 使用model模块选取最后的结果\n",
    "3. 若model模块一个都没挑出来，则选择recall模块的首选结果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 2000/2000 [03:21<00:00,  9.92it/s, 0.4053666666666667]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4053666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "def evaluation_accuracy(prediction, test_data_loader, device=None):\n",
    "    count_correct = 0.0\n",
    "    t = tqdm(test_data_loader, desc=\"Evaluation\")\n",
    "    for text, matches in t:\n",
    "        result = prediction.predict(text)\n",
    "        for r in result:\n",
    "            if (r) in matches:\n",
    "                count_correct += 1 / len(result)\n",
    "        t.set_postfix_str(count_correct / len(test_data_loader))\n",
    "    return count_correct / len(test_data_loader)\n",
    "\n",
    "print(evaluation_accuracy(p, dev_data_loader, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 10000/10000 [16:04<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "dump_file_path = \"result\\CHIP-CDN_test.json\"\n",
    "with open(test_data_path,'r',encoding=\"utf-8\") as source:\n",
    "    data = json.load(source)\n",
    "    with torch.no_grad():\n",
    "        for index,text in tqdm(enumerate(test_data_loader), desc=\"Evaluation\", total=len(test_data_loader)):\n",
    "            data[index][\"normalized_result\"] = \"##\".join(p.predict(text[0]))\n",
    "            json_result = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "with open(dump_file_path,'w',encoding=\"utf-8\") as destination:\n",
    "    destination.write(json_result)"
   ]
  }
 ]
}