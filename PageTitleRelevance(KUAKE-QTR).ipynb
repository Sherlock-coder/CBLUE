{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "f82ab70f6e4e0673e87f3f6960512c9807b8b91e8473aea7edbc34f463490a2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.任务描述\n",
    "在医疗搜索中，评估搜索词(Query)表述主题和落地页标题(Title)表述主题的匹配程度是一项重要的任务，关系到搜索结果的准确性。Query的主题是指query的专注点,用户在输入query是希望找到与query主题相关的网页。该任务需要判断Query主题和Title主题是否一致及达到多大程度上的一致，本任务数据集就是在这样的背景下产生的。\n",
    "\n",
    "## 2.任务说明\n",
    "Query和Title的相关度共分为4档（0-3），0分为最差，3分为匹配最好。\n",
    "\n",
    "- 3分：表示主题完全匹配。\n",
    "- 2分：表示主题部分匹配。\n",
    "- 1分：表示主题很少匹配，有一些参考价值。\n",
    "- 0分：表示主题完全不匹配或者没有参考价值。\n",
    "\n",
    "### 标注示例如下：\n",
    "#### 3分（高度一致/完全匹配）：\n",
    "Title主题和Query主题完全匹配。\n",
    "- Q=缺维生素b的症状\n",
    "- T=维生素b缺乏症的主要表现\n",
    "- Q=排卵期有什么症状\n",
    "- T=女性排卵期有什么症状？\n",
    "#### 2分（部分匹配，偏大或偏小）：\n",
    "Title主题略小于Query主题，Title主题是Query主题的主要方面之一。\n",
    "\n",
    "Query主题略小于Title主题，但Query主题是Title主题的主要方面之一。\n",
    "\n",
    "- Q > T\n",
    "    - Q=拉了绿色的稀便 T=拉绿色的屎怎么回事\n",
    "- Q ＜ T\n",
    "    - Q=大腿软组织损伤怎么办 T=腿部软组织损伤怎么办\n",
    "- 主题部分匹配\n",
    "    - Q=排卵期有什么症状 T=女性排卵期有什么症状？女性备孕吃什么有助排卵？\n",
    "#### 1分（很少匹配，存在一定价值）\n",
    "\n",
    "- Q=小腿抽筋是什么原因引起的 T=小腿抽筋后一直疼怎么办\n",
    "- Q=眉心长痘痘是什么原因 T=脸颊长痘痘是什么原因\n",
    "#### 0分（完全不匹配无关）\n",
    "\n",
    "- Q=挑食是什么原因造成的 T=影响身高的因素\n",
    "## 3.评测指标\n",
    "本任务的评价指标使用准确率Accuracy来评估，即：\n",
    "### 准确率(Accuracy) = #预测正确的条目数 / #预测总条目数\n",
    "\n",
    "## 4.评测数据\n",
    "本评测开放训练集数据24174条，验证集数据2913条，测试集数据5465条。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['如', '果', '这', '是', '一', '句', '话', '。']\n"
     ]
    }
   ],
   "source": [
    "# cut by word\n",
    "def tokenize(text):\n",
    "    return list(text)\n",
    "print(tokenize(\"如果这是一句话。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to sequence\n",
    "UNK_TAG = \"UNK\"\n",
    "PAD_TAG = \"PAD\"\n",
    "class Word2Sequence():\n",
    "    # 句子索引化转化类\n",
    "    # 使用：\n",
    "    # 建立词典\n",
    "    # 转换\n",
    "    # 逆转化\n",
    "\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2index_dict = {\n",
    "            UNK_TAG : self.UNK,\n",
    "            PAD_TAG : self.PAD,\n",
    "        }\n",
    "        self.count = {}\n",
    "\n",
    "\n",
    "    def fit(self, sentence):\n",
    "        # 保存句子到dict, 统计词频\n",
    "\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    "        \n",
    "\n",
    "    def build_vocab(self,min=0,max=None,max_features=None):\n",
    "        # 显式调用，建立词典\n",
    "        # min:将被采用的词至少出现min次\n",
    "        # max:将被采用的词至多出现max次\n",
    "        # max_features:按出现次数降序选择max_feature个词\n",
    "\n",
    "        self.count = {word:value for word,value in self.count.items() if value > min}\n",
    "        if(max is not None):\n",
    "            self.count = {word:value for word,value in self.count.items() if value < max}\n",
    "        if max_features is not None:\n",
    "            self.count = dict(sorted(self.count.items(), key = lambda x:x[-1], reverse=True)[:max_features])\n",
    "\n",
    "        for word in self.count:\n",
    "            self.word2index_dict[word] = len(self.word2index_dict)\n",
    "        self.index2word_dict = dict(zip(self.word2index_dict.values(), self.word2index_dict.keys()))\n",
    "\n",
    "\n",
    "    def words2index_transform(self, sentence, max_len=None):\n",
    "        # 索引化句子\n",
    "        if max_len is not None:\n",
    "            if max_len > len(sentence):\n",
    "                sentence = sentence + [PAD_TAG] * (max_len - len(sentence))\n",
    "            else:\n",
    "                sentence = sentence[:max_len]\n",
    "        return [self.word2index_dict.get(word, self.UNK) for word in sentence]\n",
    "\n",
    "\n",
    "    def index2words_transform(self, sentence):\n",
    "        # 逆索引化\n",
    "        return [self.index2word_dict.get(index) for index in sentence]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total words amount:2821\n"
     ]
    }
   ],
   "source": [
    "# dictionary build\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "train_data_path = r\"data\\KUAKE-QTR\\KUAKE-QTR_train.json\"\n",
    "test_data_path = r\"data\\KUAKE-QTR\\KUAKE-QTR_test.json\"\n",
    "dev_data_path = r\"data\\KUAKE-QTR\\KUAKE-QTR_dev.json\"\n",
    "# 通过训练集中的语料建立词典\n",
    "if(not os.path.exists(\"models/KUAKE-QTR_Word2Sequence.pkl\")):\n",
    "    word_index_tranformer = Word2Sequence()\n",
    "    with open(train_data_path, encoding=\"utf-8\") as f:\n",
    "        for data in tqdm(json.load(f)):\n",
    "            word_index_tranformer.fit(tokenize(data['query']))\n",
    "            word_index_tranformer.fit(tokenize(data['title']))\n",
    "    word_index_tranformer.build_vocab()\n",
    "    pickle.dump(word_index_tranformer, open(r\"models/KUAKE-QTR_Word2Sequence.pkl\", 'wb'))\n",
    "else:\n",
    "    word_index_tranformer = pickle.load(open(r\"models/KUAKE-QTR_Word2Sequence.pkl\", 'rb'))\n",
    "print(\"Total words amount:\" + str(len(word_index_tranformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, tensor([ 2,  3,  4,  5,  6,  6,  7,  8,  9, 10, 11,  4, 12, 13,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), tensor([ 2,  3,  4,  5, 14,  6,  6,  7, 15, 16, 17,  9, 10, 11, 12, 13, 18, 19,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])) \n 24174\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "max_sentece_length = 30\n",
    "class RosDataset(Dataset):\n",
    "    def __init__(self, data_path, train=True):\n",
    "        # 数据集准备\n",
    "        # train:是否是训练或者验证集合，不同之处在于是否会返回label\n",
    "        self.train = train\n",
    "        with open(data_path, encoding=\"utf-8\") as f:\n",
    "            self.data_list = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取索引对应位置的一条数据\n",
    "        cuted_text1 = tokenize(self.data_list[index][\"query\"])\n",
    "        cuted_text2 = tokenize(self.data_list[index][\"title\"])\n",
    "        indexed_text1 = torch.LongTensor(word_index_tranformer.words2index_transform(cuted_text1, max_len=max_sentece_length))\n",
    "        indexed_text2 = torch.LongTensor(word_index_tranformer.words2index_transform(cuted_text2, max_len=max_sentece_length))\n",
    "        if(self.train):\n",
    "            label = int(self.data_list[index][\"label\"])\n",
    "            return label, indexed_text1, indexed_text2\n",
    "        else:\n",
    "            return indexed_text1, indexed_text2\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的总数量\n",
    "        return len(self.data_list)\n",
    "\n",
    "train_dataset = RosDataset(train_data_path)\n",
    "dev_dataset = RosDataset(dev_data_path)\n",
    "test_dataset = RosDataset(test_data_path, train=False)\n",
    "print(train_dataset[0], \"\\n\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:tensor([2, 0, 2, 1, 3, 2, 3, 3, 3, 2, 3, 2, 0, 2, 3, 3, 1, 1, 3, 3, 1, 1, 3, 1,\n        3, 2, 2, 0, 0, 3, 1, 3, 3, 0, 3, 0, 0, 0, 3, 0, 3, 2, 0, 3, 1, 1, 0, 3,\n        3, 2, 3, 2, 2, 0, 3, 3, 3, 0, 3, 0, 2, 2, 2, 0, 3, 3, 2, 0, 1, 2, 3, 1,\n        3, 3, 2, 2, 2, 3, 3, 3, 1, 1, 3, 3, 2, 3, 1, 0, 2, 2, 0, 3, 2, 1, 3, 3,\n        2, 3, 3, 0, 1, 3, 0, 3, 3, 3, 3, 3, 0, 3, 1, 1, 3, 3, 2, 2, 0, 3, 3, 3,\n        0, 2, 2, 2, 3, 1, 3, 2]),tensor([[ 799, 1687,  202,  ...,    1,    1,    1],\n        [ 191,  142,   24,  ...,    1,    1,    1],\n        [  27,   54,   12,  ...,    1,    1,    1],\n        ...,\n        [ 722,  430,  108,  ...,    1,    1,    1],\n        [ 341,  342,  201,  ...,    1,    1,    1],\n        [ 701,   46,  199,  ...,    1,    1,    1]]),tensor([[343,  72, 799,  ...,   1,   1,   1],\n        [295, 199, 237,  ...,   1,   1,   1],\n        [  2,   3,  54,  ...,   1,   1,   1],\n        ...,\n        [ 73,  74,  82,  ...,   1,   1,   1],\n        [341, 342,  29,  ...,   1,   1,   1],\n        [199, 432, 701,  ...,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# 训练集中一个batch_size=128\n",
    "train_data_loader = DataLoader(dataset=train_dataset,batch_size=128,shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset,batch_size=1,shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)\n",
    "for index, (label, indexed_text1, indexed_text2) in enumerate(train_data_loader):\n",
    "    if(index > 0):\n",
    "        break\n",
    "    print(f\"{index}:{label},{indexed_text1},{indexed_text2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word_index_tranformer),embedding_dim=384,padding_idx=word_index_tranformer.PAD)\n",
    "        self.gru1 = nn.GRU(input_size=384,hidden_size=256,num_layers=3,batch_first=True,bidirectional=True)\n",
    "        self.gru2 = nn.GRU(input_size=256*4,hidden_size=256,num_layers=6,batch_first=True,bidirectional=True)\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(256*4,256),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256,64),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        mask1 = input1.eq(word_index_tranformer.PAD)\n",
    "        mask2 = input2.eq(word_index_tranformer.PAD)\n",
    "        input1 = self.embedding(input1)\n",
    "        input2 = self.embedding(input2)\n",
    "        output1,hidden_state1 = self.gru1(input1)\n",
    "        output2,hidden_state2 = self.gru1(input2)\n",
    "        \n",
    "        output1_align, output2_align = self.soft_attention_align(output1, output2, mask1, mask2)\n",
    "        output1 = torch.cat([output1, output1_align], 2)\n",
    "        output2 = torch.cat([output2, output2_align], 2)\n",
    "        \n",
    "        gru2_output1,gru2_hidden_state1 = self.gru2(output1)\n",
    "        gru2_output2,gru2_hidden_state2 = self.gru2(output2)\n",
    "        \n",
    "        out = torch.cat([gru2_output1[:,-1,:], gru2_output2[:,-1,:]], dim=-1)\n",
    "        out = self.dnn(out)\n",
    "        \n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "    def soft_attention_align(self, x1, x2, mask1, mask2):\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float(\"-inf\"))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float(\"-inf\"))\n",
    "\n",
    "        attention_weight = x1.bmm(x2.transpose(1, 2))\n",
    "        x1_weight = F.softmax(attention_weight + mask2.unsqueeze(1), dim=-1) \n",
    "        x2_output = x1_weight.bmm(x2)\n",
    "\n",
    "        x2_weight = F.softmax(attention_weight.transpose(1, 2) + mask1.unsqueeze(1), dim=-1) \n",
    "        x1_output = x2_weight.bmm(x1)\n",
    "        \n",
    "        return x1_output, x2_output"
   ]
  },
  {
   "source": [
    "# GRU+Attention Siamese Network\n",
    "![](resrc\\GRU+Attention.png)\n",
    "## 说明\n",
    "- embeddinding层将索引表示转化为长度为384的vector，图中没有画出\n",
    "- 带上横线标识这是一个max_sentence长度的矩阵\n",
    "- GRU1_A和GRU1_B、GRU2_A和GRU2_B共享权重\n",
    "    - 实现上其实就是两个句子过了同一个网络\n",
    "    - 孪生神经网络\n",
    "        - 将两个句子映射到语义空间中衡量相似度\n",
    "            - idea:转变句子在语义空间的表现来贴合题目分类的题意\n",
    "        - 可以是直接取最后输出的embedding vector用Contrastive Loss做二分类问题\n",
    "            - [Manhattan LSTM Model](file:///D:/Files/NLP%20Project/works/TianChi/paper/KUAKE/Siamese%20Recurrent%20Architectures%20for%20Learning%20Sentence%20Similarity.pdf)\n",
    "            - Contrastive Loss的思想就是让相似的离的近，不相似的离的远\n",
    "            - idea:扩展Contrastive Loss来适应特殊的n分类问题\n",
    "                - 原题的4分类其实不是简单相似程度由高到低，而是还附带一定的语义信息\n",
    "                - TODO:通过设计Loss来使得对句向量的相似性的衡量更符合题意\n",
    "        - 这里因为是四分类问题所以直接用了FC+softmax\n",
    "            - [Siamese Recurrent Neural Network](file:///D:/Files/NLP%20Project/works/TianChi/paper/KUAKE/Learning%20Text%20Similarity%20with%20Siamese%20Recurrent%20Networks.pdf)\n",
    "            - 可以是FC层得到句子的embedding vector再做衡量\n",
    "            - 这里是用FC来做classifier\n",
    "- soft attention是两个句子的互注意力\n",
    "    - 值得注意的是在该层中padding被用-inf填充，因而在attention中没有权重，即忽略了padding\n",
    "## 实验\n",
    "- 通过加深GRU的层数能让在验证集的正确率上涨十几个点\n",
    "- 但超过差不多10层之后反而会让表现出现急剧的下降\n",
    "    - 还有一个值得注意的细节：\n",
    "        - 5+5：28%\n",
    "        - 3+6：55%\n",
    "- 在其他任务类似的模型中有pooling层，但它的效果我没有做验证"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def train(epochs, model, model_path=None, optimizer_path=None, device=None):\n",
    "    # device: 模型运行的位置\n",
    "    # model_path:保存或加载的模型的模型路径\n",
    "    # optimizer_path:保存或加载的模型的优化器路径\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    t = tqdm(range(epochs), desc=\"Train\")\n",
    "    for epoch in t:\n",
    "        for index, (label, text1, text2) in enumerate(train_data_loader):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text1, text2)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            t.set_description(f\"epoch:{epoch}\")\n",
    "            t.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if not model_path is None:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    if not optimizer_path is None:\n",
    "        torch.save(optimizer.state_dict(), optimizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_accuracy(model, test_data_loader, device=None):\n",
    "    # model:衡量准确率的model\n",
    "    # test_data_loader:做准确衡量的data_loader\n",
    "    # device:运行的设备位置\n",
    "    count_correct = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for label, text1, text2 in tqdm(dev_data_loader, desc=\"Evaluation\"):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            if(model(text1, text2).argmax() == label):\n",
    "                count_correct = count_correct + 1\n",
    "    print(f\"\\n{count_correct}/{len(test_data_loader)}\")\n",
    "    return count_correct / len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 2913/2913 [02:07<00:00, 22.81it/s]\n",
      "1608/2913\n",
      "\n",
      "0.5520082389289392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mode = False\n",
    "model = SiameseNetwork()\n",
    "model_path = \"models\\QTR_SiameseNetwork.pth\"\n",
    "optimizer_path = \"models\\QTR_SiameseNetwork_optim.pth\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(train_mode):\n",
    "    train(epochs=37, model=model, model_path=model_path, optimizer_path=optimizer_path, device=device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "print('\\n' + str(evaluation_accuracy(model, dev_data_loader, device=device)))"
   ]
  },
  {
   "source": [
    "- 曾经调出来过0.60几左右的准确率过\n",
    "- 但是我把参数改了以后忘记保存了\n",
    "- 而由于这个模型比较大训练一次要三四十分钟\n",
    "- 就不想重新调了orz"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: 100%|██████████| 5465/5465 [04:52<00:00, 18.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# dump to json\n",
    "dump_file_path = \"result\\KUAKE-QTR_test.json\"\n",
    "with open(test_data_path,'r',encoding=\"utf-8\") as source:\n",
    "    data = json.load(source)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index,(text1, text2) in tqdm(enumerate(test_data_loader), desc=\"Evaluation\", total=len(test_data_loader)):\n",
    "            if not device is None:\n",
    "                label = label.to(device)\n",
    "                text1 = text1.to(device)\n",
    "                text2 = text2.to(device)\n",
    "            data[index][\"label\"] = str(model(text1, text2).argmax().item())\n",
    "            json_result = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "with open(dump_file_path,'w',encoding=\"utf-8\") as destination:\n",
    "    destination.write(json_result)"
   ]
  }
 ]
}